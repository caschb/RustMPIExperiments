# -*- org-confirm-babel-evaluate: nil; -*-
#+title: Rust MPI Experiments
#+author: Christian Asch

* Setting up repositories

#+begin_src bash :results output :exports both
git submodule update --init --recursive
#+end_src

This will clone the following repos:

+ [[https://gitlab.com/CNCA_CeNAT/bs-solctra-implementations.git][BS-Solctra Implementations]]: The original C++ MPI+OpenMP
  implementation of BS-Solctra
+ [[https://github.com/caschb/bs-solctra-mpi-rs][BS-Solctra-mpi-rs]]: My Rust+Rayon+MPI implementation of BS-Solctra
+ [[https://github.com/caschb/Kernels][Kernels]]: My fork of the Parallel Research Kernels which include the
  Rust MPI code of the PIC simulation



** Setting up C compiler for Kernels
#+begin_src bash :dir Kernels/common
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
cp make.defs.gcc make.defs
sed -i 's/^MPIDIR=.*/MPIDIR=\/opt\/tools\/mpich-3.3.2-gcc-9.3.0/' make.defs
#+end_src

#+RESULTS:

* Compiling BS-SOLCTRA MPI

We have to change the Makefile so it works on the =kura= nodes

#+begin_src bash :results output :exports both
FILE=bs-solctra-implementations/Makefile
sed -i 's/^VECT_FLAGS=.*/VECT_FLAGS=-fopenmp/' $FILE
#+end_src

#+begin_src bash :results output :exports both :dir bs-solctra-implementations
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
make
#+end_src

#+RESULTS:
: mpicxx -O3 -std=c++11 -fopenmp -o bs-solctra-multinode solctra_multinode.h solctra_multinode.cpp main_multinode.cpp utils.h utils.cpp
: cp bs-solctra-multinode results
: rm bs-solctra-multinode;

* Compiling BS-SOLCTRA Rust+MPI

#+begin_src bash :results output :exports both :dir bs-solctra-mpi-rs
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
. /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
cargo build -r -j 15
#+end_src


* Compiling PIC C+MPI

#+begin_src bash :results output :exports both :dir Kernels/MPI1/PIC-static
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
make pic
#+end_src

#+RESULTS:
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c pic.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/MPI_bail_out.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/wtime.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/random_draw.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -o pic   -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI pic.o MPI_bail_out.o wtime.o random_draw.o  -lm

* Compiling PIC Rust+MPI

#+begin_src bash :results output :exports both :dir Kernels/RUST/pic-mpi
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
. /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
cargo build -r
#+end_src

#+RESULTS:

* Experiments

** Common components for SLURM files

*** Header

We prepare the heading of all SLURM Files
#+begin_src bash :noweb-ref header
#SBATCH --partition=kura-all
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err
#SBATCH --time=12:00:00
#SBATCH --ntasks-per-node=20
#SBATCH --cpus-per-task=1
#SBATCH --exclusive
#+end_src

*** Load modules

#+begin_src bash :noweb-ref modules
module purge
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
#+end_src

*** Kura module fix

#+begin_src bash :noweb-ref kura-fix
. /opt/Modules/3.2.10/init/sh
#+end_src

** BS-SOLCTRA MPI+OpenMP Weak Scaling

*** Job Name

#+begin_src bash :noweb-ref bsmo-name
#SBATCH --job-name solc-cpp-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsmo-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
export REPETITIONS=10
TOTAL_PARTICLES=$((${SLURM_NNODES}*256))
for (( i = 0; i < ${REPETITIONS}; i++))
do
    mpiexec -n ${SLURM_NNODES}\
	    ./bs-solctra-multinode\
	    -length ${TOTAL_PARTICLES}\
	    -particles input_1000.txt\
	    -id ${SLURM_JOB_ID}_${i}\
	    -resource resources/\
	    -mode 1\
	    -magnetic_prof 0 100 0 2\
	    -print_typef 1\
	    -steps 1000
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir sol-mpi-wk/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir sol-mpi-wk/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir sol-mpi-wk/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 8 nodes
#+begin_src bash :dir sol-mpi-wk/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 12 nodes
#+begin_src bash :dir sol-mpi-wk/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src


** BS-SOLCTRA MPI+OpenMP Strong Scaling

*** Job Name
#+begin_src bash :noweb-ref bsms-name
#SBATCH --job-name solc-cpp-st
#+end_src
*** Execution commands

#+begin_src bash :noweb-ref bsms-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
TOTAL_PARTICLES=$((12*256))
REPETITIONS=10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    mpiexec -n ${SLURM_NNODES}\
	    ./bs-solctra-multinode\
	    -length ${TOTAL_PARTICLES}\
	    -particles input_1000.txt\
	    -id ${SLURM_JOB_ID}_${i}\
	    -resource resources/\
	    -mode 1\
	    -magnetic_prof 0 100 0 2\
	    -print_typef 1\
	    -steps 1000
done
#+end_src


*** Assemble Slurm files for strong scaling and copy files

**** 1 node

#+begin_src bash :dir sol-mpi-st/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir sol-mpi-st/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir sol-mpi-st/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 8 nodes
#+begin_src bash :dir sol-mpi-st/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 12 nodes
#+begin_src bash :dir sol-mpi-st/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src


** BS-SOLCTRA MPI+Rayon Weak Scaling

*** Job Name

#+begin_src bash :noweb-ref bsrw-name
#SBATCH --job-name solc-rust-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsrw-command
export RAYON_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
TOTAL_PARTICLES=$((${SLURM_NNODES}*256))
REPETITIONS=10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    RUST_LOG=info mpiexec -n ${SLURM_NNODES}\
		  ./bs-solctra-rs\
		  --num-particles ${TOTAL_PARTICLES}\
		  --particles-file input_1000.csv\
		  --resource-path resources/\
		  --mode 1\
		  --magprof 0\
		  --steps 1000\
		  --output out_${SLURM_JOBID}_${i}
done
#+end_src


*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir sol-rst-wk/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 2 node

#+begin_src bash :dir sol-rst-wk/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src


**** 4 nodes

#+begin_src bash :dir sol-rst-wk/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 8 nodes

#+begin_src bash :dir sol-rst-wk/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 12 nodes

#+begin_src bash :dir sol-rst-wk/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12

<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src






** BS-SOLCTRA MPI+Rayon Strong Scaling

*** Job Name

#+begin_src bash :noweb-ref bsrs-name
#SBATCH --job-name solc-rust-st
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsrs-command
export RAYON_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
TOTAL_PARTICLES=$((12*256))
REPETITIONS = 10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    RUST_LOG=info mpiexec -n ${SLURM_NNODES}\
		  ./bs-solctra-rs\
		  --num-particles ${TOTAL_PARTICLES}\
		  --particles-file input_1000.csv\
		  --resource-path resources/\
		  --mode 1\
		  --magprof 0\
		  --steps 1000\
		  --output out_${SLURM_JOBID}_${i}
done
#+end_src


*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir sol-rst-st/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 2 node

#+begin_src bash :dir sol-rst-st/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src


**** 4 nodes

#+begin_src bash :dir sol-rst-st/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 8 nodes

#+begin_src bash :dir sol-rst-st/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 12 nodes

#+begin_src bash :dir sol-rst-st/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12

<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

* Submit experiments

** BS-SOLCTRA MPI+OpenMP Weak Scaling

*** 1 node

#+begin_src bash :dir sol-mpi-wk/1
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234704

*** 2 nodes

#+begin_src bash :dir sol-mpi-wk/2
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234705

*** 4 nodes

#+begin_src bash :dir sol-mpi-wk/4
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234706

*** 8 nodes

#+begin_src bash :dir sol-mpi-wk/8
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234707

*** 12 nodes
#+begin_src bash :dir sol-mpi-wk/12
rm -rf results_* solc-* stdout*
ls
#+end_src

#+begin_src bash :dir sol-mpi-wk/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234120

*** Clean all results
#+begin_src bash :dir sol-mpi-wk
rm -rf ./*/results_* ./*/solc-* ./*/stdout* ./*/stats.csv
#+end_src

#+RESULTS:


** BS-SOLCTRA MPI+OpenMP Strong Scaling

*** 1 node

#+begin_src bash :dir sol-mpi-st/1
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234169

*** 2 nodes

#+begin_src bash :dir sol-mpi-st/2
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234170

*** 4 nodes

#+begin_src bash :dir sol-mpi-st/4
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234171

*** 8 nodes

#+begin_src bash :dir sol-mpi-st/8
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234172

*** 12 nodes

#+begin_src bash :dir sol-mpi-st/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234173

*** Clean all results
#+begin_src bash :dir sol-mpi-st
rm ./*/results_* ./*/solc-* stdout*
#+end_src

** BS-SOLCTRA Rust MPI+Rayon Weak Scaling
*** 1 node

#+begin_src bash :dir sol-rst-wk/1
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234688

*** 2 nodes

#+begin_src bash :dir sol-rst-wk/2
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234689

*** 4 nodes

#+begin_src bash :dir sol-rst-wk/4
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234690

*** 8 nodes

#+begin_src bash :dir sol-rst-wk/8
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234691

*** 12 nodes

#+begin_src bash :dir sol-rst-wk/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234692

*** Clean all results
#+begin_src bash :dir sol-rst-wk
rm -rf ./*/out_* ./*/solc-* stdout*
#+end_src

#+RESULTS:

** BS-SOLCTRA Rust MPI+Rayon Strong Scaling
*** 1 node

#+begin_src bash :dir sol-rst-st/1
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234694

*** 2 nodes

#+begin_src bash :dir sol-rst-st/2
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234695

*** 4 nodes

#+begin_src bash :dir sol-rst-st/4
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234696

*** 8 nodes

#+begin_src bash :dir sol-rst-st/8
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234697

*** 12 nodes

#+begin_src bash :dir sol-rst-st/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234698

*** Clean all results
#+begin_src bash :dir sol-rst-st
rm -rf ./*/out_* ./*/solc-* stdout*
#+end_src

#+RESULTS:


** Check queue
#+begin_src shell
squeue -u casch
#+end_src

#+RESULTS:
|  JOBID | PARTITION | NAME     | USER  | ST | TIME | NODES | NODELIST(REASON)                                    |
| 234697 | kura-all  | solc-rus | casch | PD | 0:00 |     8 | (Resources)                                         |
| 234692 | kura-all  | solc-rus | casch | PD | 0:00 |    12 | (Priority)                                          |
| 234698 | kura-all  | solc-rus | casch | PD | 0:00 |    12 | (Priority)                                          |
| 234694 | kura-all  | solc-rus | casch | R  | 0:28 |     1 | kura-1a.cnca                                        |
| 234695 | kura-all  | solc-rus | casch | R  | 0:22 |     2 | kura-1b.cnca,kura-1c.cnca                           |
| 234696 | kura-all  | solc-rus | casch | R  | 0:16 |     4 | kura-1d.cnca,kura-2a.cnca,kura-2b.cnca,kura-2c.cnca |

