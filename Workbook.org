# -*- org-confirm-babel-evaluate: nil; -*-
#+title: Rust MPI Experiments
#+author: Christian Asch

* Setting up repositories

#+begin_src bash :results output :exports both
git submodule update --init --recursive
#+end_src

This will clone the following repos:

+ [[https://gitlab.com/CNCA_CeNAT/bs-solctra-implementations.git][BS-Solctra Implementations]]: The original C++ MPI+OpenMP
  implementation of BS-Solctra
+ [[https://github.com/caschb/bs-solctra-mpi-rs][BS-Solctra-mpi-rs]]: My Rust+Rayon+MPI implementation of BS-Solctra
+ [[https://github.com/caschb/Kernels][Kernels]]: My fork of the Parallel Research Kernels which include the
  Rust MPI code of the PIC simulation



** Setting up C compiler for Kernels
#+begin_src bash :dir Kernels/common
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
cp make.defs.gcc make.defs
sed -i 's/^MPIDIR=.*/MPIDIR=\/opt\/tools\/mpich-3.3.2-gcc-9.3.0/' make.defs
#+end_src

#+RESULTS:

* Compiling BS-SOLCTRA MPI

We have to change the Makefile so it works on the =kura= nodes

#+begin_src bash :results output :exports both
FILE=bs-solctra-implementations/Makefile
sed -i 's/^VECT_FLAGS=.*/VECT_FLAGS=-fopenmp/' $FILE
#+end_src

#+begin_src bash :results output :exports both :dir bs-solctra-implementations
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
make
#+end_src

#+RESULTS:
: mpicxx -O3 -std=c++11 -fopenmp -o bs-solctra-multinode solctra_multinode.h solctra_multinode.cpp main_multinode.cpp utils.h utils.cpp
: cp bs-solctra-multinode results
: rm bs-solctra-multinode;

* Compiling BS-SOLCTRA Rust+MPI

#+begin_src bash :results output :exports both :dir bs-solctra-mpi-rs
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
. /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
cargo build -r -j 15
#+end_src

* Compiling PIC C+MPI

#+begin_src bash :results output :exports both :dir Kernels/MPI1/PIC-static
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
make pic
#+end_src

#+RESULTS:
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c pic.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/MPI_bail_out.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/wtime.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI -DVERBOSE=0   -DRESTRICT_KEYWORD=0  -I../../include -c ../../common/random_draw.c
: /opt/tools/mpich-3.3.2-gcc-9.3.0/bin/mpicc -o pic   -O3 -mtune=native -ffast-math -g3 -Wall   -DMPI pic.o MPI_bail_out.o wtime.o random_draw.o  -lm

* Compiling PIC Rust+MPI

#+begin_src bash :results output :exports both :dir Kernels/RUST/pic-mpi
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
. /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
cargo build -r
#+end_src

#+RESULTS:

* Experiments
** Common components for SLURM files

*** Header

We prepare the heading of all SLURM Files
#+begin_src bash :noweb-ref header
#SBATCH --partition=kura-all
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err
#SBATCH --time=12:00:00
#SBATCH --ntasks-per-node=20
#SBATCH --cpus-per-task=1
#SBATCH --exclusive
#+end_src

*** Load modules

#+begin_src bash :noweb-ref modules
module purge
module load mpich/3.3.2-gcc-9.3.0 gcc/9.3.0
#+end_src

*** Kura module fix

#+begin_src bash :noweb-ref kura-fix
. /opt/Modules/3.2.10/init/sh
#+end_src

** BS-SOLCTRA MPI+OpenMP Weak Scaling

*** Job Name

#+begin_src bash :noweb-ref bsmo-name
#SBATCH --job-name solc-cpp-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsmo-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
export REPETITIONS=10
TOTAL_PARTICLES=$((${SLURM_NNODES}*256))
for (( i = 0; i < ${REPETITIONS}; i++))
do
    mpiexec -n ${SLURM_NNODES}\
	    ./bs-solctra-multinode\
	    -length ${TOTAL_PARTICLES}\
	    -particles input_1000.txt\
	    -id ${SLURM_JOB_ID}${i}\
	    -resource resources/\
	    -mode 1\
	    -magnetic_prof 0 100 0 2\
	    -print_typef 1\
	    -steps 1000
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir sol-mpi-wk/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir sol-mpi-wk/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir sol-mpi-wk/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 8 nodes
#+begin_src bash :dir sol-mpi-wk/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

**** 12 nodes
#+begin_src bash :dir sol-mpi-wk/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<bsmo-name>>
<<kura-fix>>
<<modules>>
<<bsmo-command>>
#+end_src

** BS-SOLCTRA MPI+OpenMP Strong Scaling

*** Job Name
#+begin_src bash :noweb-ref bsms-name
#SBATCH --job-name solc-cpp-st
#+end_src
*** Execution commands

#+begin_src bash :noweb-ref bsms-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
TOTAL_PARTICLES=$((12*256))
REPETITIONS=10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    mpiexec -n ${SLURM_NNODES}\
	    ./bs-solctra-multinode\
	    -length ${TOTAL_PARTICLES}\
	    -particles input_1000.txt\
	    -id ${SLURM_JOB_ID}${i}\
	    -resource resources/\
	    -mode 1\
	    -magnetic_prof 0 100 0 2\
	    -print_typef 1\
	    -steps 1000
done
#+end_src


*** Assemble Slurm files for strong scaling and copy files

**** 1 node

#+begin_src bash :dir sol-mpi-st/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir sol-mpi-st/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir sol-mpi-st/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 8 nodes
#+begin_src bash :dir sol-mpi-st/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

**** 12 nodes
#+begin_src bash :dir sol-mpi-st/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-mpi-st/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<bsms-name>>
<<kura-fix>>
<<modules>>
<<bsms-command>>
#+end_src

** BS-SOLCTRA MPI+Rayon Weak Scaling

*** Job Name

#+begin_src bash :noweb-ref bsrw-name
#SBATCH --job-name solc-rust-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsrw-command
export RAYON_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
TOTAL_PARTICLES=$((${SLURM_NNODES}*256))
REPETITIONS=10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    RUST_LOG=info mpiexec -n ${SLURM_NNODES}\
		  ./bs-solctra-rs\
		  --num-particles ${TOTAL_PARTICLES}\
		  --particles-file input_1000.csv\
		  --resource-path resources/\
		  --mode 1\
		  --magprof 0\
		  --steps 1000\
		  --output results_${SLURM_JOBID}${i}
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files
**** 1 node

#+begin_src bash :dir sol-rst-wk/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 2 node

#+begin_src bash :dir sol-rst-wk/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir sol-rst-wk/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 8 nodes

#+begin_src bash :dir sol-rst-wk/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

**** 12 nodes

#+begin_src bash :dir sol-rst-wk/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12

<<bsrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrw-command>>
#+end_src

** BS-SOLCTRA MPI+Rayon Strong Scaling

*** Job Name

#+begin_src bash :noweb-ref bsrs-name
#SBATCH --job-name solc-rust-st
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref bsrs-command
export RAYON_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
TOTAL_PARTICLES=$((12*256))
REPETITIONS = 10
for (( i = 0; i < ${REPETITIONS}; i++))
do
    RUST_LOG=info mpiexec -n ${SLURM_NNODES}\
		  ./bs-solctra-rs\
		  --num-particles ${TOTAL_PARTICLES}\
		  --particles-file input_1000.csv\
		  --resource-path resources/\
		  --mode 1\
		  --magprof 0\
		  --steps 1000\
		  --output out_${SLURM_JOBID}_${i}
done
#+end_src


*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir sol-rst-st/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 2 node

#+begin_src bash :dir sol-rst-st/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src


**** 4 nodes

#+begin_src bash :dir sol-rst-st/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 8 nodes

#+begin_src bash :dir sol-rst-st/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src

**** 12 nodes

#+begin_src bash :dir sol-rst-st/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle sol-rst-st/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12

<<bsrs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<bsrs-command>>
#+end_src



** PIC C Weak Scaling

*** Job Name
#+begin_src bash :noweb-ref piccw-name
#SBATCH --job-name pic-c-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref piccw-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
export REPETITIONS=10
TOTAL_STEPS=100
TOTAL_PARTICLES=$((${SLURM_NNODES}*102400))
for (( i = 0; i < ${REPETITIONS}; i++))
do
    echo "~rep: ${i}~" >&2
    echo "GEOMETRIC" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 2\
	    GEOMETRIC 0.99
    echo "###################################"
    echo "SINUSOIDAL" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    0 1\
	    SINUSOIDAL
    echo "###################################"
    echo "LINEAR" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 0\
	    LINEAR\
	    1.0\
	    3.0
    echo "###################################"
    echo "PATCH" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 0\
	    PATCH\
	    0\
	    200\
	    100\
	    200
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir pic-c-wk/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<piccw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccw-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir pic-c-wk/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<piccw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccw-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir pic-c-wk/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<piccw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccw-command>>
#+end_src


**** 8 nodes

#+begin_src bash :dir pic-c-wk/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<piccw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccw-command>>
#+end_src


**** 12 nodes

#+begin_src bash :dir pic-c-wk/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<piccw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccw-command>>
#+end_src

** PIC C Strong Scaling

*** Job Name
#+begin_src bash :noweb-ref piccs-name
#SBATCH --job-name pic-c-st
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref piccs-command
export OMP_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
export OMP_SCHEDULE=dynamic
export REPETITIONS=10
TOTAL_STEPS=100
TOTAL_PARTICLES=$((12*102400))
for (( i = 0; i < ${REPETITIONS}; i++))
do
    echo "~rep: ${i}~" >&2
    echo "GEOMETRIC" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 2\
	    GEOMETRIC 0.99
    echo "###################################"
    echo "SINUSOIDAL" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    0 1\
	    SINUSOIDAL
    echo "###################################"
    echo "LINEAR" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 0\
	    LINEAR\
	    1.0\
	    3.0
    echo "###################################"
    echo "PATCH" >&2
    time mpiexec -n ${SLURM_NNODES}\
	    ./pic\
	    ${TOTAL_STEPS}\
	    1000\
	    ${TOTAL_PARTICLES}\
	    1 0\
	    PATCH\
	    0\
	    200\
	    100\
	    200
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :dir pic-c-st/1/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-st/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<piccs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccs-command>>
#+end_src

**** 2 nodes

#+begin_src bash :dir pic-c-st/2/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-st/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<piccs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccs-command>>
#+end_src

**** 4 nodes

#+begin_src bash :dir pic-c-st/4/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-st/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<piccs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccs-command>>
#+end_src

**** 8 nodes

#+begin_src bash :dir pic-c-st/8/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-st/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<piccs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccs-command>>
#+end_src

**** 12 nodes

#+begin_src bash :dir pic-c-st/12/ :mkdirp yes
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
#+end_src

#+RESULTS:


#+begin_src bash :shebang #!/bin/bash :tangle pic-c-st/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<piccs-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<piccs-command>>
#+end_src

** PIC RUST Weak Scaling

*** Job Name
#+begin_src bash :noweb-ref picrw-name
#SBATCH --job-name pic-r-ws
#+end_src

*** Execution commands

#+begin_src bash :noweb-ref picrw-command
export RAYON_NUM_THREADS=${SLURM_NTASKS_PER_NODE}
REPETITIONS=10
TOTAL_STEPS=100
TOTAL_PARTICLES=$((${SLURM_NNODES}*102400))
for (( i = 0; i < ${REPETITIONS}; i++))
do
    echo "~rep: ${i}~" >&2
    echo "GEOMETRIC" >&2
    time mpiexec -n ${SLURM_NNODES}\
	 ./pic-mpi\
	 -i ${TOTAL_STEPS}\
	 -g 1000\
	 -t ${TOTAL_PARTICLES}\
	 -p 1 -v 2\
	 geometric\
	 -a 0.99
    echo "###################################"
    echo "SINUSOIDAL" >&2
    time mpiexec -n ${SLURM_NNODES}\
	 ./pic-mpi\
	 -i ${TOTAL_STEPS}\
	 -g 1000\
	 -t ${TOTAL_PARTICLES}\
	 -p 1 -v 2\
	 sinusoidal
    echo "###################################"
    echo "LINEAR" >&2
    time mpiexec -n ${SLURM_NNODES}\
	 ./pic-mpi\
	 -i ${TOTAL_STEPS}\
	 -g 1000\
	 -t ${TOTAL_PARTICLES}\
	 -p 1 -v 2\
	 linear\
	 -n 1.0\
	 -c 3.0
    echo "###################################"
    echo "PATCH" >&2
    time mpiexec -n ${SLURM_NNODES}\
	 ./pic-mpi\
	 -i ${TOTAL_STEPS}\
	 -g 1000\
	 -t ${TOTAL_PARTICLES}\
	 -p 1 -v 2\
	 patch\
	 --xleft 0 --xright 200\
	 --ybottom 100 --ytop 200
done
#+end_src

*** Assemble Slurm files for weak scaling and copy files

**** 1 node

#+begin_src bash :shebang #!/bin/bash :tangle pic-r-wk/1/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 1
<<picrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<picrw-command>>
#+end_src

**** 2 nodes

#+begin_src bash :shebang #!/bin/bash :tangle pic-r-wk/2/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 2
<<picrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<picrw-command>>
#+end_src

**** 4 nodes

#+begin_src bash :shebang #!/bin/bash :tangle pic-r-wk/4/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 4
<<picrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<picrw-command>>
#+end_src

**** 8 nodes

#+begin_src bash :shebang #!/bin/bash :tangle pic-r-wk/8/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 8
<<picrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<picrw-command>>
#+end_src

**** 12 nodes

#+begin_src bash :shebang #!/bin/bash :tangle pic-r-wk/12/run.slurm :mkdirp yes :noweb yes
<<header>>
#SBATCH --nodes 12
<<picrw-name>>
<<kura-fix>>
<<modules>>
source /work/casch/spack/share/spack/setup-env.sh
spack env activate rustdev
<<picrw-command>>
#+end_src

** PIC Rust Strong Scaling
* Submit experiments
** BS-SOLCTRA MPI+OpenMP Weak Scaling
*** 1 node

#+begin_src bash :dir sol-mpi-wk/1
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235007

*** 2 nodes

#+begin_src bash :dir sol-mpi-wk/2
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235010

*** 4 nodes

#+begin_src bash :dir sol-mpi-wk/4
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235011

*** 8 nodes

#+begin_src bash :dir sol-mpi-wk/8
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235020

*** 12 nodes
#+begin_src bash :dir sol-mpi-wk/12
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
rm -rf results_* solc-* stdout*
ls
#+end_src

#+begin_src bash :dir sol-mpi-wk/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234120

*** Clean all results
#+begin_src bash :dir sol-mpi-wk
rm -rf ./*/results_* ./*/solc-* ./*/stdout* ./*/stats.csv
#+end_src

#+RESULTS:

** BS-SOLCTRA MPI+OpenMP Strong Scaling

*** 1 node

#+begin_src bash :dir sol-mpi-st/1
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235023

*** 2 nodes

#+begin_src bash :dir sol-mpi-st/2
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235024

*** 4 nodes

#+begin_src bash :dir sol-mpi-st/4
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235025

*** 8 nodes

#+begin_src bash :dir sol-mpi-st/8
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235027

*** 12 nodes

#+begin_src bash :dir sol-mpi-st/12
rm -rf results_* *.err *.out stats.csv stdout*
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/input_1000.txt .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-implementations/results/bs-solctra-multinode .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234173

** BS-SOLCTRA Rust MPI+Rayon Weak Scaling
*** 1 node

#+begin_src bash :dir sol-rst-wk/1
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235029

*** 2 nodes

#+begin_src bash :dir sol-rst-wk/2
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
#+end_src

#+RESULTS:

#+begin_src bash :dir sol-rst-wk/2
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235030

*** 4 nodes

#+begin_src bash :dir sol-rst-wk/4
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235031

*** 8 nodes

#+begin_src bash :dir sol-rst-wk/8
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235032

*** 12 nodes

#+begin_src bash :dir sol-rst-wk/12
rm -r out_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234692

** BS-SOLCTRA Rust MPI+Rayon Strong Scaling
*** 1 node

#+begin_src bash :dir sol-rst-st/1
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235035

*** 2 nodes

#+begin_src bash :dir sol-rst-st/2
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235036

*** 4 nodes
#+begin_src bash :dir sol-rst-st/4
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234696

*** 8 nodes

#+begin_src bash :dir sol-rst-st/8
rm -r results_* *.err *.out
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/input_1000.csv .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/tests/test-resources/resources .
ln -sf ~/RustMPIExperiments/bs-solctra-mpi-rs/target/release/bs-solctra-rs .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235037

*** 12 nodes

#+begin_src bash :dir sol-rst-st/12
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234698

** PIC C Weak Scaling

*** 1 node

#+begin_src bash :dir pic-c-wk/1
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235038

*** 2 nodes

#+begin_src bash :dir pic-c-wk/2
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235039

*** 4 nodes

#+begin_src bash :dir pic-c-wk/4
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235040

*** 8 nodes

#+begin_src bash :dir pic-c-wk/8
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235041

*** 12 nodes
#+begin_src bash :dir pic-c-wk/12
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234120

** PIC C Strong Scaling

*** 1 node

#+begin_src bash :dir pic-c-st/1
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235042

*** 2 nodes

#+begin_src bash :dir pic-c-st/2
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235043

*** 4 nodes

#+begin_src bash :dir pic-c-st/4
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235044

*** 8 nodes

#+begin_src bash :dir pic-c-st/8
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235045

*** 12 nodes

#+begin_src bash :dir pic-c-st/12
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234120

** PIC RUST Weak Scaling

*** 1 node

#+begin_src bash :dir pic-r-wk/1
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/RUST/pic-mpi/target/release/pic-mpi .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 235005

*** 2 nodes

#+begin_src bash :dir pic-c-wk/2
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234705

*** 4 nodes

#+begin_src bash :dir pic-c-wk/4
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234706

*** 8 nodes

#+begin_src bash :dir pic-c-wk/8
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234707

*** 12 nodes
#+begin_src bash :dir pic-c-wk/12
rm *.err *.out
ln -sf ~/RustMPIExperiments/Kernels/MPI1/PIC-static/pic .
sbatch run.slurm
#+end_src

#+RESULTS:
: Submitted batch job 234120

